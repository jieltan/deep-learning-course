{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C Tutorial\n",
    "\n",
    "In this tutorial we will train an agent using Advantage Actor-Critic (A2C) for the Pendulum-v0 task from `OpenAI Gym <https://gym.openai.com/>`__.\n",
    "\n",
    "**Task**\n",
    "\n",
    "The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright. You can find an\n",
    "official leaderboard with various algorithms and visualizations at the\n",
    "[Gym website](https://gym.openai.com/envs/Pendulum-v0).\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/8510097/31701471-726f54c0-b385-11e7-9f05-5c50f2affbb4.PNG\" alt=\"Pendulum\" style=\"width: 400px;\"/>\n",
    "\n",
    "This is a continuous control task where the action is a continuous variable of the joint effort. The reward is cost funtion of the observation, and the lowest cost is -16.2736044, while the highest loss is 0. So the reward is always negative. In essence, the goal is maximize the reward, to remain at zero angle (vertical), with the least rotational velocity, and the least effort. More details can be found [here](https://github.com/openai/gym/wiki/Pendulum-v0)\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "We will implement an A2C algorithm for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def _action(self, action):\n",
    "        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        action = action * 2 - 1\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor and Critic networks**\n",
    "\n",
    "For continuous control tasks, the input into the policy (actor) is the state observation, and we assume the policy $\\pi(a|s)$ follows a Gaussian distribution $N(\\mu(s), \\sigma(s))$. The parameters of the Gaussian are estimated using a policy network. Actions can then be sampled from this distribution.\n",
    "\n",
    "The critic network will take a state as input, and output a value function estimate of the input state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "## Implement the Actor and Critic networks\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "\n",
    "        ### TODO ###\n",
    "        ## Define layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        ### TODO ###\n",
    "        ## Implement forward pass\n",
    "        \n",
    "        return mu, sigma_sq\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        ### TODO ###\n",
    "        ## Define layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        ### TODO ###\n",
    "        ## Define layers\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor and Critic network losses**\n",
    "\n",
    "You will implement the loss functions for the actor and critic networks below.\n",
    "\n",
    "Recall policy gradients,\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\tau;\\theta)}[r(\\tau)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\text{log} \\pi_\\theta (a_t^i|s_t^i) A^\\pi (s_t^i, a_t^i)\n",
    "$$\n",
    "where $N, T$ represent the number of agent trajectories and episode length, respectively.\n",
    "\n",
    "We will use n-step rewards for estimating the Q-function in the advantage as follows. \n",
    "\n",
    "Let \n",
    "$$\n",
    "y_t^i = \\left( \\sum_{t'=t}^{t+N-1} \\gamma^{t'-t}r(s_{t'}^i, a_{t'}^i) \\right) + \\gamma^N V_\\phi^\\pi(s_{t+N}^i) \n",
    "$$\n",
    "\n",
    "We estimate the advantage as\n",
    "$$\n",
    "A^\\pi (s_{t}^i, a_{t}^i) =  y_t^i - V_\\phi^\\pi(s_t^i)\n",
    "$$\n",
    "\n",
    "The loss function $\\mathcal{L}_\\theta$ for the actor network is given by\n",
    "$$\n",
    "\\mathcal{L}_\\theta = -\\sum_{t=1}^T \\text{log} \\pi_\\theta(a_t|s_t) A^\\pi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "The critic network is trained to regress to the targets $y_t^i$. The loss function $\\mathcal{L}_\\phi$ for the critic network is given by\n",
    "$$\n",
    "\\mathcal{L}_\\phi = \\sum_{t=1}^T (V_\\phi^\\pi(s_t^i) - y_t^i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space, device):\n",
    "        self.action_space = action_space\n",
    "        self.actor = Actor(hidden_size, num_inputs, action_space)\n",
    "        self.critic = Critic(hidden_size, num_inputs, action_space)\n",
    "        self.actor = self.actor.to(device)\n",
    "        self.critic = self.critic.to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        self.device = device\n",
    "\n",
    "    def normal(self, x, mu, sigma_sq):\n",
    "        pi = Variable(torch.FloatTensor([math.pi])).to(self.device)\n",
    "        a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "        b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "        return a*b\n",
    "\n",
    "   \n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.actor(Variable(state).to(self.device))\n",
    "        state_value = self.critic(Variable(state).to(self.device))\n",
    "        #softplus is smooth approximation of RELU to constrain the output to be positive\n",
    "        sigma_sq = F.softplus(sigma_sq) \n",
    "\n",
    "        eps = torch.randn(mu.size())\n",
    "        # calculate the probability\n",
    "        action = mu + sigma_sq.sqrt()*Variable(eps)\n",
    "        action = action.to(device).data\n",
    "        prob = self.normal(action, mu, sigma_sq)\n",
    "        pi = Variable(torch.FloatTensor([math.pi])).to(self.device)\n",
    "        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
    "\n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, state_value, entropy\n",
    "\n",
    "    def compute_loss(self, rewards, log_probs, state_values, entropies, gamma, Nsteps):\n",
    "\n",
    "        ### TODO ###\n",
    "        ## Implement the Actor and Critic losses\n",
    "\n",
    "        self.actor_loss = None\n",
    "        self.critic_loss = None\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        for loss, optimizer in [(self.actor_loss, self.actor_optimizer), \n",
    "                                (self.critic_loss, self.critic_optimizer)]:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "According to the pseudo code, we will interact with the environment based on the action from policy network, and collect the episode information to update the actor and critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env = NormalizedActions(env)\n",
    "env.seed(498)\n",
    "torch.manual_seed(498)\n",
    "np.random.seed(498)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "agent = A2C(hidden_size=128, num_inputs=env.observation_space.shape[0], action_space=env.action_space, device=device)\n",
    "reward_list = []\n",
    "for i_episode in range(3000):\n",
    "    state = torch.Tensor([env.reset()])\n",
    "    entropies = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    state_values = []\n",
    "    for t in range(1000):\n",
    "        ## TODO: given the state, get the action from the policy network,\n",
    "        ## take the action in the environment, put the entropy,log probability\n",
    "        ## and reward value into the corresponding list, update the state\n",
    "        action, log_prob, state_value, entropy = agent.select_action(state)\n",
    "        action = action.cpu()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.numpy()[0])\n",
    "\n",
    "        entropies.append(entropy)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state_values.append(state_value)\n",
    "        state = torch.Tensor([next_state])\n",
    "        if done:\n",
    "            break\n",
    "        #if i_episode % 100 == 0:\n",
    "        #    env.render()\n",
    "    agent.compute_loss(rewards, log_probs, state_values, entropies, gamma=0.99, Nsteps=10)\n",
    "    agent.update_parameters()\n",
    "    reward_list.append(np.sum(rewards))\n",
    "    print(\"Episode: {}, reward: {}\".format(i_episode, np.sum(rewards)))\n",
    "    \n",
    "env.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "reward_list = torch.FloatTensor(reward_list)\n",
    "plt.figure(2)\n",
    "plt.plot(reward_list.numpy())\n",
    "plt.title('Training...')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "if len(reward_list) >= 100:\n",
    "    means = reward_list.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    means = torch.cat((torch.ones(99)*means[0], means))\n",
    "    plt.plot(means.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compare A2C with REINFORCE\n",
    "2. How does the N above in N-step rewards affect the gradient variance ?\n",
    "3. Compare DDPG with A2C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
